{
  # Spark relation config
  spark: {


    driver: {
      cores: 1
      maxResultSize: 1G
    }

    cores {
      max: 16
    }
  }

  # Nebula Graph relation config
  nebula: {
    address:{
      graph:["192.168.8.172:3799"]
      meta:["192.168.8.172:45509"]
    }
    user: user
    pswd: password
    space: exchange

    connection {
      timeout: 3000
      retry: 3
    }

    execution {
      retry: 3
    }

    error: {
      max: 32
      # failed import job will be recorded in output path
      output: /tmp/errors
    }

    rate: {
      limit: 1024
      timeout: 1000
    }
  }

  # Processing tags
  tags: [

    # Loading tag from HDFS and data type is csv
    {
      name: person
      type: {
        source: csv
        sink: client
      }
      path: "hdfs://192.168.8.171:9000/ldbc/tags/person.csv"
      fields: [firstName]
      nebula.fields: [first_name]
      vertex: firstName
      separator: "|"
      header: true
      batch: 256
      partition: 32
    }

     # Loading tag from neo4j
    {
      name: person1
      type: {
        source: neo4j
        sink: client
      }
      server: "bolt://192.168.8.164:7687"
      user: neo4j
      password: nebula
      exec: "match (n:team) return n.name as name"
      fields: [name]
      nebula.fields: [name]
      vertex: {
        field:name
        # nebula 2.0 does not support int vid yet, so do not config policy.
        # policy:uuid
      }
      partition: 10
      batch: 1000
      check_point_path: /tmp/test
    }
  ]

  edges:[
  {
        name: knows1
        type: {
          source: csv
          sink: client
        }
        path: "hdfs://192.168.8.171:9000/ldbc/edges/person_knows_person.csv"
        fields: [creationDate]
        nebula.fields: [timep]
        source: Person.id1
        target: Person.id2
        separator: "|"
        header: true
        batch: 256
        partition: 32
      }
  ]
}
