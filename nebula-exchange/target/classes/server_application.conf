{
  # Spark relation config
  spark: {
    driver: {
      cores: 1
      maxResultSize: 1G
    }

    cores {
      max: 16
    }
  }

  # if the hive is in the same cluster with spark, you can ignore this hive configure
  hive: {
    waredir: "hdfs://HOSTNAME:9000/apps/svr/hive-0.13.1-cdh5.3.2/warehouse/"
    connectionURL: "jdbc:mysql://your_ip:3306/hive_spark?characterEncoding=UTF-8"
    connectionDriverName: "com.mysql.jdbc.Driver"
    connectionUserName: "user"
    connectionPassword: "password"
  }

  # Nebula Graph relation config
  nebula: {
    address:{
      graph:["127.0.0.1:3699"]
      meta:["127.0.0.1:45500"]
    }
    user: user
    pswd: password
    space: test

    connection {
      timeout: 3000
      retry: 3
    }

    execution {
      retry: 3
    }

    error: {
      max: 32
      # failed import job will be recorded in output path
      output: /tmp/errors
    }

    rate: {
      limit: 1024
      timeout: 1000
    }
  }

  # Processing tags
  tags: [
      # Loading from Hive
     {
        name: tag-name-1
        type: {
          source: hive
          sink: client
        }
        exec: "select hive-field0, hive-field1, hive-field2 from database.table"
        fields: [hive-field-0, hive-field-1, hive-field-2]
        nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
        vertex: {
          field: hive-field-0
          # nebula 2.0 does not support int vid yet, so do not config policy.
          # policy: "hash"
        }
        vertex: hive-field-0
        batch: 256
        partition: 32
     }

     # Loading tag from neo4j
    {
      name: tag-name-0
      type: {
        source: neo4j
        sink: client
      }
      server: "bolt://127.0.0.1:7687"
      user: neo4j
      password: neo4j
      exec: "match (n:label) return n.neo4j-field-0 as neo4j-field-0, n.neo4j-field-1 as neo4j-field-1 order by (n.neo4j-field-0)"
      fields: [neo4j-field-0, neo4j-field-1]
      nebula.fields: [nebula-field-0, nebula-field-1]
      vertex: {
        field:neo4j-field-0
        # nebula 2.0 does not support int vid yet, so do not config policy.
        # policy:uuid
      }
      partition: 10
      batch: 1000
      check_point_path: /tmp/test
    }

    # Loading from HBase, if fields or vertex contains rowkey, please configure it as rowkey.
    {
      name: hbase-table-name
      type: {
        source: hbase
        sink: client
      }
      host:127.0.0.1
      port:2181
      table:hbase-table
      columnFamily:hbase-table-cloumnfamily
      fields: [hbase-column-0, hbase-column-1]
      nebula.fields: [nebula-field-0, nebula-field-1]
      vertex: rowkey
      partition: 10
      batch: 1000
    }
  ]

  # Processing edges
  edges: [
    # Loading from Hive
    {
      name: edge-name-1
      type: {
        source: hive
        sink: client
      }
      exec: "select hive-field0, hive-field1, hive-field2 from database.table"
      fields: [ hive-field-0, hive-field-1, hive-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      source: hive-field-0
      target: hive-field-1
      batch: 256
      partition: 32
    }

    # Loading from neo4j
    {
      name: edge-name-0
      type: {
        source: neo4j
        sink: client
      }
      server: "bolt://127.0.0.1:7687"
      user: neo4j
      password: neo4j
      exec: "match (a:vertex_label)-[r:edge_label]->(b:vertex_label) return a.neo4j-source-field, b.neo4j-target-field, r.neo4j-field-0 as neo4j-field-0, r.neo4j-field-1 as neo4j-field-1 order by id(r)"
      fields: [neo4j-field-0, neo4j-field-1]
      nebula.fields: [nebula-field-0, nebula-field-1]
      source: {
        field: a.neo4j-source-field
      }
      target: {
        field: b.neo4j-target-field
      }
      partition: 10
      batch: 1000
      check_point_path: /tmp/test
    }

    # Loading from hbase
    {
      name: hbase-table-name
      type: {
        source: hbase
        sink: client
      }
      host:127.0.0.1
      port:2181
      table:hbase-table
      columnFamily:hbase-table-cloumnfamily
      fields: [hbase-column-0, hbase-column-1]
      nebula.fields:[nebula-field-0, nebula-field-1]
      source: {
        field: hbase-column-k
      }
      target: {
        field: hbase-column-h
      }
      partition: 10
      batch: 1000
    }
  ]
}
